{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47c08e5",
   "metadata": {},
   "source": [
    "<h1> Wasserstein GAN\n",
    "<h2> Sonstige Prüfungsleistung im Modul Angewandte Programmierung \n",
    "<h3> Thema: Anime GAN  \n",
    "<h4> Autorin: Kim-Kristin Mähl (582494)\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1701.07875.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a602c6d",
   "metadata": {},
   "source": [
    "Wasserstein GAN\n",
    "Vorteile:\n",
    "    - bessere Traings-Stabilität als DCGAN\n",
    "Nachteile:\n",
    "    - Längeres training\n",
    "\n",
    "Idee beim GAN:\n",
    "    - Distanz zwischen prob generate und prob realistic soll so klein\n",
    "    wie möglich sein, um möglichst realistische Bilder zu generieren\n",
    "\n",
    "Problem:\n",
    "    - Wie wird die Distanz zwischen den beiden prob Distributionen\n",
    "    beschrieben/ definiert\n",
    "\n",
    "Lösung:\n",
    "    - Wasserstein Distance -> WGAN nutzt Wasserstein Distance für den Loss\n",
    "    (- Kullback-Leiber (KL) divergence)\n",
    "    (- Jensen-Shannon (JS) divergence) <- Equivalent zum GAN Loss\n",
    "        -> Hat Gradienten Probleme die das Training instabil werden lassen\n",
    "\n",
    "WGAN:\n",
    "    - Disc. Maximierung von Expression\n",
    "    - Gen.  Minimierung von Expression\n",
    "\n",
    "Default- Werte im WGAN\n",
    "    - LR = 0.00005\n",
    "    - Clipping Parameter = 0.01\n",
    "    - Batch Size = 64\n",
    "    - n_critic = 5 (Anzahl der Iterationen von critic pro generation iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b73b66",
   "metadata": {},
   "source": [
    "<h2> Download \"Opendatasets\" und \"Torchsummary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326446e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opendatasets # Opendataset installieren > eine Python-Bibliothek zum Herunterladen von Datensätzen aus Online-Quellen wie Kaggle und Google Drive.\n",
    "!pip install torchsummary # Download von Torchsummary für Prüfung Generator und Critic/ Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff06b2",
   "metadata": {},
   "source": [
    "<h2> Importieren der Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9b614",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # Loss\n",
    "from torchvision.utils import save_image  # Speichern von Bildern\n",
    "import torch.optim as optim  # Optimierungs-Algorithmen\n",
    "import torch.nn as nn  # Neuronales Netz\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt  # plotten von Grafen/ Bildern\n",
    "import torchvision.transforms as transforms  # Transformieren von Bildern\n",
    "import torchvision.datasets as ImageFolder\n",
    "import torch.utils.data as DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import os # Dient zum lokalen Speichern des Datasets\n",
    "import opendatasets as od\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5717a69",
   "metadata": {},
   "source": [
    "<h2> Definieren von Parametern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3a637",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64  # Größe der Bilder\n",
    "BATCH_SIZE = 64  # Anzahl der Batches\n",
    "WORKERS = 2  # Anzahl der Kerne beim Arbeiten auf der GPU\n",
    "# Normalisierung mit 0.5 Mittelwert und Standardabweichung für alle drei Channels der Bilder\n",
    "NORM = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "NUM_EPOCH = 20  # Anzahl der Epochen\n",
    "LR = 0.00005  # Learningrate\n",
    "LATENT_SIZE = 100  # Radom Input für den Generator\n",
    "N_CRTIC = 5\n",
    "WEIGHT_CLIPPING = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c153e6",
   "metadata": {},
   "source": [
    "<h2> Download des Datasets von Kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee44375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jovian.ai/ahmadyahya11/pytorch-gans-anime\n",
    "# Ordner für den Download anlegen\n",
    "data_dir = '../data/'\n",
    "os.makedirs(data_dir, exist_ok=True)  # Anlegen eines Ordners für Bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add9b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erklärung zum Umgang mit Opendata und Kaggle - https://pypi.org/project/opendatasets/\n",
    "# Datensatz:Anime-Faces werden von Kaggle geladen\n",
    "# Hierfür wird der User-API-KEY benötigt\n",
    "# APIKEY {\"username\":\"XXXX\",\"key\":\"XXXXX\"}\n",
    "dataset_url = 'https://www.kaggle.com/splcher/animefacedataset'\n",
    "# Images werden in './animefacedataset' gespeichert\n",
    "od.download(dataset_url, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Ordner in \"../data/\"\n",
    "print(os.listdir(data_dir))  # zeigt Ordner an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426061eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gibt 10 Bezeichnungen von Bildern aus\n",
    "print(os.listdir(data_dir+'animefacedataset/images')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d064b7",
   "metadata": {},
   "source": [
    "<h2> Vorbereiten& Erstellen des Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d956ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "transform = transforms.Compose([\n",
    "    # Resize der Images auf 64 der kürzesten Seite; Andere Seite wird\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    # skaliert, um das Seitenverhältnis des Bildes beizubehalten.\n",
    "    # Zuschneiden auf die Mitte des Images, sodass ein quadratisches Bild mit 64 x 64 Pixeln entsteht\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    # Umwandeln in einen Tensor (Bildern in numerische Werte umwandeln)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*NORM)])          # Normalisierung Mean & Standardabweichung von 0.5 für alle Channels\n",
    "# (Anzahl: 3 für farbige Bilder)\n",
    "# Pixelwerte liegen damit zwischen (-1;1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bffc1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\"\"\"\n",
    "ImageFolder() : Befehl erwartet, dass nach Images nach labeln organisiert sind (root/label/picture.png)\n",
    "\"\"\"\n",
    "org_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4858a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "\"\"\"\n",
    "Dataloader(): ermöglicht zufällige Stichproben der Daten auszugeben;\n",
    "Dient dazu, dass das Modell nicht mit dem gesamten Dataset umgehen muss > Training effizienter\n",
    "\"\"\"\n",
    "org_loader = t.utils.data.DataLoader(org_dataset,           # Dataset (Images)\n",
    "                                     batch_size=BATCH_SIZE, # Es wird auf Batches trainiert, damit auf Basis eines Batch-Fehlers das NN angepasst wird\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18f8a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<h2> Abfrage des Devices (CPU o. GPU) und Laden des Tensors auf das jeweilige verfügbare Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nutzen der GPU wenn vorhanden, ansonsten CPU\n",
    "def get_default_device():\n",
    "    if t.cuda.is_available():     # Wenn cuda verfügbar dann:\n",
    "        return t.device('cuda')   # Nutze Device = Cuda (=GPU)\n",
    "    else:                         # Ansonsten\n",
    "        return t.device('cpu')    # Nutze Device = CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10014ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeigen welches Device verfügbar ist\n",
    "device = get_default_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48987c13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "*Hilfsklasse zum Verschieben des Dataloaders \"org_loader\" auf das jeweilige Device*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109aa58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "\n",
    "    # Initialisierung\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    # Anzahl der Images pro Batch\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    # Erstellt einen Batch an Tensoren nach dem Verschieben auf das Device\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            yield tuple(tensor.to(self.device) for tensor in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea300703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader auf dem verfügbaren Device schieben\n",
    "org_loader = DeviceDataLoader(org_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d288f4",
   "metadata": {},
   "source": [
    "<h2> Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60526912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(t.nn.Module):\n",
    "    \"\"\"\n",
    "    Generator 1 Input Layer; 3 Hidden Layer ; 1 Output Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "            # Output = (inputsize - 1)*stride - 2*padding + (kernelsite-1)+1\n",
    "            # ConvTranspose2d hilft dabei aus einem kleinen\n",
    "            nn.ConvTranspose2d(LATENT_SIZE, IMAGE_SIZE*8, 4, 1, 0, bias=False),\n",
    "            # hilft einen größeren Tensor zu erstellen (Bezogen auf Channels)\n",
    "            nn.BatchNorm2d(IMAGE_SIZE*8),\n",
    "            nn.ReLU(inplace=True),  # Relu lässt keine negativen werte zu\n",
    "\n",
    "            nn.ConvTranspose2d(IMAGE_SIZE*8, IMAGE_SIZE * \\\n",
    "                               4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(IMAGE_SIZE*4),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(IMAGE_SIZE*4, IMAGE_SIZE * \\\n",
    "                               2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(IMAGE_SIZE*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(IMAGE_SIZE*2, IMAGE_SIZE, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(IMAGE_SIZE),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(IMAGE_SIZE, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()   # (-1 und 1) ; Tanh wird häufig verwendet da eine begrenzte Aktivierung es dem Modell ermöglicht,\n",
    "                        # schneller zu lernen. (https://arxiv.org/pdf/1511.06434.pdf S. 3)\n",
    "\n",
    "            # Output: 3 x 64 x 64\n",
    "        )\n",
    "\n",
    "    # Feedforward\n",
    "    def forward(self, input):\n",
    "        output = self.generator(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e41030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen des Generators\n",
    "NN_Generator = Generator().to(device)\n",
    "print(NN_Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59b6ea",
   "metadata": {},
   "source": [
    "<h2> Critic/ Diskriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22685ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator (t.nn.Module):\n",
    "    \"\"\"\n",
    "    Diskriminator 1 Input Layer; 3 Hidden Layer ; 1 Output Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            # Output = ((inputsize) + 2*padding + (kernelsite-1)-1/stride) -1\n",
    "            nn.Conv2d(3, IMAGE_SIZE, 4, 2, 1, bias=False),   # conv2d hilft dabei aus einem großem Tensor einen kleinen Tensor zu stellen\n",
    "            nn.LeakyReLU(0.2, inplace=True),    # Leaky RELU lässt negative Werte zu (nicht wie RELU); Neuronen werden somit nicht auf Null gesetzt\n",
    "                                                # Hilft dem Generator, da dieser nur \"Lernen\" kann wenn er vom Diskriminator einen Gradienten erhält\n",
    "\n",
    "            # state size. (IMAGE_SIZE) x 32 x 32\n",
    "            nn.Conv2d(IMAGE_SIZE, IMAGE_SIZE * 2, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(IMAGE_SIZE * 2, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # state size. (IMAGE_SIZE*2) x 16 x 16\n",
    "            nn.Conv2d(IMAGE_SIZE * 2, IMAGE_SIZE * 4, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(IMAGE_SIZE * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # nn.Dropout2d(0.1),  #Dropout\n",
    "\n",
    "            # state size. (IMAGE_SIZE*4) x 8 x 8\n",
    "            nn.Conv2d(IMAGE_SIZE * 4, IMAGE_SIZE * 8, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(IMAGE_SIZE * 8, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "\n",
    "            # state size. (IMAGE_SIZE*8) x 4 x 4\n",
    "            nn.Conv2d(IMAGE_SIZE * 8, 1, 4, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    # Feedforward\n",
    "    def forward(self, input):\n",
    "        output = self.discriminator(input)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2df960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen des Critics/ Diskriminators\n",
    "NN_Discriminator = Discriminator().to(device)\n",
    "print(NN_Discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad2f25",
   "metadata": {},
   "source": [
    "*Torchsummary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary - Generator\")\n",
    "summary(NN_Generator, (LATENT_SIZE, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1817dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary - Critic/Discriminator\")\n",
    "summary(NN_Discriminator, (3, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088218b",
   "metadata": {},
   "source": [
    "<h2> Gewichtsinitialisierung von Generator und Critic/ Diskriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean 0 and Standardabweichung 0.02\n",
    "def w_initial(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        t.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        t.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        t.nn.init.constant_(m.bias, val=0)\n",
    "        \n",
    "# Gewichtsinitialisierung von Generator und Critic/ Discriminator        \n",
    "NN_Generator = NN_Generator.apply(w_initial)\n",
    "NN_Discriminator = NN_Discriminator.apply(w_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6ff65",
   "metadata": {},
   "source": [
    "*Hilfsfunktionen zur Normalisierung von Tensoren und grafischen Darstellung*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensornormalisierung\n",
    "def tensor_norm(img_tensors):\n",
    "    return img_tensors * NORM[1][0] + NORM[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafische Darstellung\n",
    "def show_images(images, nmax=64):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.title(\"Fake_Images\")\n",
    "    ax.imshow(make_grid(tensor_norm(images.detach()[:nmax]), nrow=8).permute(\n",
    "        1, 2, 0).cpu())     # detach() : erstellt eine neue \"Ansicht\",\n",
    "                            # sodass diese Operationen nicht mehr verfolgt werden,\n",
    "                            # d. h. der Gradient wird nicht berechnet und der Untergraph\n",
    "                            # wird nicht aufgezeichnet > Speicher wird nicht verwendet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992eff0",
   "metadata": {},
   "source": [
    "*Radom Tensor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6412cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator --> Input: Random Tensor\n",
    "# Generator --> Output: Fake-Images (Batchsize, data_dim, Pixel, Pixel)\n",
    "random_Tensor = t.randn(BATCH_SIZE, LATENT_SIZE, 1, 1, device=device)\n",
    "fake_images = NN_Generator(random_Tensor)\n",
    "print(fake_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fce74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufruf der Funktion \"Show_Images\" mit Fake Images\n",
    "show_images(fake_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03872a4",
   "metadata": {},
   "source": [
    "<h2> Trainieren des Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb90a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train(Gen_Opt):\n",
    "\n",
    "    #Gradienten = 0\n",
    "    Gen_Opt.zero_grad()\n",
    "\n",
    "    # Generierung von Fake-Images\n",
    "    fake_img = NN_Generator(random_Tensor)\n",
    "\n",
    "    # Übergeben der Fakes-Images an den Diskriminator (Versuch den Diskriminator zu täuschen)\n",
    "    pred = NN_Discriminator(fake_img)\n",
    "\n",
    "    loss = -t.mean(pred)\n",
    "\n",
    "    # Backprop und Update der Gewichte des Generators\n",
    "    loss.backward()\n",
    "    Gen_Opt.step()\n",
    "\n",
    "    #print(\"Training Gen\")\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ece856",
   "metadata": {},
   "source": [
    "<h2> Trainieren des Diskriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c845fc",
   "metadata": {
    "comment_questions": false
   },
   "outputs": [],
   "source": [
    "def disc_train(real_images, Dis_Opt):\n",
    "\n",
    "    #Gradienten = 0\n",
    "    Dis_Opt.zero_grad()\n",
    "\n",
    "    # Generieren von Fake Images\n",
    "    fake = NN_Generator(random_Tensor).to(device)\n",
    "\n",
    "    # Reale Bilder werden an den Diskriminator übergeben\n",
    "    pred_real = NN_Discriminator(real_images)\n",
    "    # print(pred_real.size())\n",
    "\n",
    "    # Fake Bilder werden an den Diskriminator übergeben\n",
    "    pred_fake = NN_Discriminator(fake).to(device)\n",
    "\n",
    "    # Berechnung des Loss\n",
    "    loss_critic = -(t.mean(pred_real)-t.mean(pred_fake))\n",
    "    \n",
    "    # Backprop & Update Optimzer\n",
    "    loss_critic.backward(retain_graph=True)\n",
    "    Dis_Opt.step()\n",
    "\n",
    "    #print(\"Training disc\")\n",
    "    return loss_critic.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d90fd",
   "metadata": {},
   "source": [
    "*Ordner anlegen für die vom Generator erstellten Images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_gen_samples = '../data/outputs/'\n",
    "#os.makedirs('../outputs/dir_gen_samples', exist_ok=True)\n",
    "os.makedirs(dir_gen_samples, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Funktion zum speichern der vom generierten Bilder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965683e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saves_gen_samples(idx, random_Tensor):\n",
    "\n",
    "    # Randomisierter Tensor wird an den Generator übergeben\n",
    "    fake_img = NN_Generator(random_Tensor)\n",
    "\n",
    "    # Setzen von Bildbezeichnungen für die Fake_Images\n",
    "    fake_img_name = \"gen_img-{0:0=4d}.png\".format(idx)\n",
    "\n",
    "    # Tensor-Normalisierung; Speichern der Fake_Images im Ordner \"Outputs/dir_gen_samples/\"\n",
    "    save_image(tensor_norm(fake_img), os.path.join(\n",
    "        dir_gen_samples, fake_img_name), nrow=8)\n",
    "        \n",
    "    show_images(fake_img)  # Plotten der Fake_Images\n",
    "    print(\"Gespeichert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fcef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufruf der Funktion\n",
    "saves_gen_samples(0, random_Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78801d",
   "metadata": {},
   "source": [
    "<h2> Zentrale Training-Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7feabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Critic/Discriminator, Generator, Anzahl Epochen, Learning Rate\n",
    "def train(NN_Discriminator, NN_Generator, NUM_EPOCH, LR, start_idx=1):\n",
    "    t.cuda.empty_cache()  # leert den Cache, wenn auf der GPU gearbeitet wird\n",
    "\n",
    "    # Initierung des Trainings\n",
    "    NN_Discriminator.train()\n",
    "    NN_Generator.train()\n",
    "\n",
    "    # Listen für Übersicht des Fortschritts\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "\n",
    "    # Optimzer (=RMS Prop)\n",
    "    Gen_Opt = t.optim.RMSprop(NN_Generator.parameters(),\n",
    "                              lr=LR)\n",
    "    Dis_Opt = t.optim.RMSprop(NN_Discriminator.parameters(),\n",
    "                              lr=LR)\n",
    "\n",
    "    # Iteration über die Epochen\n",
    "    for epoch in range(0, NUM_EPOCH):\n",
    "\n",
    "        # Iteration über die Bilder\n",
    "        for i, (img_real, _) in enumerate(org_loader):\n",
    "\n",
    "            # Iteration über Anzahl Critic (=5)    \n",
    "            for _ in range(N_CRTIC):\n",
    "                # Trainieren des Diskrimniators\n",
    "                d_loss = disc_train(img_real, Dis_Opt)\n",
    "            \n",
    "                # Weight Clipping (= 0.01) - Gesamtgradient des Diskriminators dadurch begrenzt, dass die einzelnen Gewichte im Diskriminator separat begrenzt werden.\n",
    "                for p in NN_Discriminator.parameters():\n",
    "                    p.data.clamp_(-WEIGHT_CLIPPING, WEIGHT_CLIPPING)\n",
    "\n",
    "            # Trainieren des Generators\n",
    "            g_loss = gen_train(Gen_Opt)\n",
    "\n",
    "            Count = i  # Index/ Iterationen zählen\n",
    "            print(\"index:\", i, \"D_loss:\", d_loss, \"G_Loss:\", g_loss) # Ausgabe Loss pro Iteration über die Bilder (Zwischenprüfung)\n",
    "\n",
    "        # Speichern des Gesamtlosses von Critic/ Diskriminator und Generator\n",
    "        D_losses.append(d_loss)\n",
    "        G_losses.append(g_loss)\n",
    "\n",
    "\n",
    "        # Ausgabe EPOCH, Loss: Gen und Critic/ Disc \n",
    "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}\".format(\n",
    "            epoch+1, NUM_EPOCH, g_loss, d_loss)) \n",
    "\n",
    "        # Speichern der generierten Samples/ Images\n",
    "        saves_gen_samples(epoch+start_idx, random_Tensor)\n",
    "\n",
    "    return G_losses, D_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc577e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufruf der Trainingsfunktion (Diskriminator/Critic & Generator mit der LR & Anzahl der Epochen)\n",
    "G_losses, D_losses = train(NN_Discriminator, NN_Generator, NUM_EPOCH, LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff294a3",
   "metadata": {},
   "source": [
    "<h2> Grafische Darstellung des Loss von Generator und Critic/ Discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e15fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Darstellung Loss \n",
    "EPOCH_COUNT_G= range(1,len(G_losses)+1) # Anzahl der Epochen vom Gen.\n",
    "EPOCH_COUNT_D= range(1,len(D_losses)+1) # Anzahl der Epochen vom Crtic/ Dis.\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"LOSS: Generator und Critic / Discriminator während dem Training\")\n",
    "plt.plot(EPOCH_COUNT_G, G_losses,\"r-\", label=\"Gen\")\n",
    "plt. plot(EPOCH_COUNT_D,D_losses,\"b-\", label=\"Critic\")\n",
    "plt.xlabel(\"EPOCH\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbe1a3cfd8a5058f1abdf68edb1ce1f2b89102bbafa6eec62f902d8120c37b9c"
  },
  "jupytext": {
   "cell_metadata_filter": "comment_questions,-all",
   "formats": "ipynb,py",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38_torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
