{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Wasserstein GAN with GP\n",
    "<h2> Sonstige Prüfungsleistung im Modul Angewandte Programmierung \n",
    "<h3> Thema: Anime GAN  \n",
    "<h4> Autorin: Kim-Kristin Mähl (582494)\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1704.00028.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Download von \"Opendatasets\" und \"Torchsummary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (0.1.20)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from opendatasets) (8.0.3)\n",
      "Requirement already satisfied: kaggle in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from opendatasets) (1.5.12)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from opendatasets) (4.62.3)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (2021.10.8)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (2.26.0)\n",
      "Requirement already satisfied: urllib3 in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.10 in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (1.16.0)\n",
      "Requirement already satisfied: python-slugify in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (5.0.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from requests->kaggle->opendatasets) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from requests->kaggle->opendatasets) (2.0.9)\n",
      "Requirement already satisfied: torchsummary in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets # Opendataset installieren > eine Python-Bibliothek zum Herunterladen von Datensätzen aus Online-Quellen wie Kaggle und Google Drive.\n",
    "!pip install torchsummary # Download von Torchsummary für Prüfung Generator und Critic/ Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Importieren der Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from tkinter.tix import IMAGE\n",
    "from matplotlib import image\n",
    "from torchvision.utils import save_image  # Speichern von Bildern\n",
    "import torch.optim as optim  # Optimierungs-Algorithmen\n",
    "import torch.nn as nn  # Neuronales Netz\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt  # plotten von Grafen/ Bildern\n",
    "import torchvision.transforms as transforms  # Transformieren von Bildern\n",
    "import torchvision.datasets as ImageFolder\n",
    "import torch.utils.data as DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import os                 # Dient zum lokalen Speichern des Datasets\n",
    "import opendatasets as od\n",
    "from random import random, weibullvariate\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Definition der Parameter/ Variablen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64  # Größe der Bilder\n",
    "BATCH_SIZE = 64  # Anzahl der Batches\n",
    "WORKERS = 2  # Anzahl der Kerne beim Arbeiten auf der GPU\n",
    "# Normalisierung mit 0.5 Mittelwert und Standardabweichung für alle drei Channels der Bilder\n",
    "NORM = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) # Normalisierung Channels\n",
    "WORKERS = 2  # Anzahl der Kerne beim Arbeiten auf der GPU\n",
    "NUM_EPOCH = 40  # Anzahl der Epochen\n",
    "LR = 1e-4  # Learningrate\n",
    "LATENT_SIZE = 100  # Radom Input für den Generator\n",
    "N_CRITIC = 5 # Anzahl Iteration über Critic\n",
    "LAMBDA_GP = 10  # Penalty Koeffizient\n",
    "no_of_channels = 3 # Anzahl der Channels (RGB > 3 Channels)\n",
    "cur_step = 0 # Variable zum zählen\n",
    "display_step = 500 # Anzahl der Interationen nach dem ein Image angezeigt werden soll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Download des Datasets von Kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"../data/animefacedataset\" (use force=True to force download)\n",
      "['animefacedataset', '.gitkeep', 'outputs']\n",
      "['4426_2003.jpg', '38921_2012.jpg', '55591_2016.jpg', '8777_2004.jpg', '56274_2017.jpg', '24208_2008.jpg', '13759_2006.jpg', '19302_2007.jpg', '14698_2006.jpg', '30569_2010.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Anlegen eines Ordners für Bilder\n",
    "data_dir = '../data/'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Erklärung zum Umgang mit Opendata und Kaggle - https://pypi.org/project/opendatasets/\n",
    "# Datensatz:Anime-Faces werden von Kaggle geladen\n",
    "# Hierfür wird der User-API-KEY benötigt\n",
    "# APIKEY {\"username\":\"XXXX\",\"key\":\"XXXXX\"}\n",
    "dataset_url = 'https://www.kaggle.com/splcher/animefacedataset'\n",
    "# Images werden in './animefacedataset' gespeichert\n",
    "od.download(dataset_url, data_dir)\n",
    "\n",
    "# zeigt Ordner unter \"../data/\" an\n",
    "print(os.listdir(data_dir))  \n",
    "\n",
    "# gibt 10 Bezeichnungen von Bildern aus (Prüfung ob Bilder geladen worden)\n",
    "print(os.listdir(data_dir+'animefacedataset/images')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Vorbereiten& Erstellen des Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "transform = transforms.Compose([\n",
    "    # Resize der Images auf 64 der kürzesten Seite; Andere Seite wird\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    # skaliert, um das Seitenverhältnis des Bildes beizubehalten.\n",
    "    # Zuschneiden auf die Mitte des Images, sodass ein quadratisches Bild mit 64 x 64 Pixeln entsteht\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    # Umwandeln in einen Tensor (Bildern in numerische Werte umwandeln)\n",
    "    transforms.ToTensor(),\n",
    "    # Normalisierung Mean & Standardabweichung von 0.5 für alle Channels\n",
    "    # Anzahl: 3 für farbige Bilder\n",
    "    # Pixelwerte liegen damit zwischen (-1;1)\n",
    "    transforms.Normalize(*NORM)])          \n",
    "\n",
    "\n",
    "# Dataset\n",
    "\"\"\"\n",
    "ImageFolder() : Befehl erwartet, dass nach Images nach labeln organisiert sind (root/label/picture.png)\n",
    "\"\"\"\n",
    "org_dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Dataloader\n",
    "\"\"\"\n",
    "Dataloader(): ermöglicht zufällige Stichproben der Daten auszugeben;\n",
    "Dient dazu, dass das Modell nicht mit dem gesamten Dataset umgehen muss > Training effizienter\n",
    "\"\"\"\n",
    "org_loader = t.utils.data.DataLoader(org_dataset,               # Dataset (Images)\n",
    "                                     batch_size=BATCH_SIZE,     # Es wird auf Batches trainiert, damit auf Basis eines Batch-Fehlers das NN angepasst wird\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Abfrage des Devices (CPU o. GPU) und Laden des Tensors auf das jeweilige verfügbare Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Nutzen der GPU wenn vorhanden, ansonsten CPU\n",
    "\n",
    "def get_default_device():\n",
    "    if t.cuda.is_available():     # Wenn cuda verfügbar dann:\n",
    "        return t.device('cuda')   # Nutze Device = Cuda (=GPU)\n",
    "    else:                         # Ansonsten\n",
    "        return t.device('cpu')    # Nutze Device = CPU\n",
    "\n",
    "\n",
    "# Anzeigen welches Device verfügbar ist\n",
    "device = get_default_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hilfsklasse zum Verschieben des Dataloaders \"org_loader\" auf das jeweilige Device*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "\n",
    "    # Initialisierung\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    # Anzahl der Images pro Batch\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    # Erstellt einen Batch an Tensoren nach dem Verschieben auf das Device\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            yield tuple(tensor.to(self.device) for tensor in batch)\n",
    "\n",
    "\n",
    "# Dataloader auf dem verfügbaren Device\n",
    "dataloader = DeviceDataLoader(org_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Randomisierter Tensor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(n_samples, noise_dim, device=device):    \n",
    "    return  torch.randn(n_samples,noise_dim, 1,1,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (generator): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator 1 Input Layer; 3 Hidden Layer ; 1 Output Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, no_of_channels=no_of_channels, noise_dim=LATENT_SIZE, gen_dim=IMAGE_SIZE):\n",
    "      super(Generator, self).__init__()\n",
    "      self.generator = nn.Sequential(\n",
    "          # Output = (inputsize - 1)*stride - 2*padding + (kernelsite-1)+1\n",
    "          # ConvTranspose2d hilft dabei aus einem kleinen\n",
    "          nn.ConvTranspose2d(noise_dim, gen_dim*8, 4, 1, 0, bias=False),\n",
    "          nn.BatchNorm2d(gen_dim*8),\n",
    "          nn.ReLU(True),\n",
    "\n",
    "          nn.ConvTranspose2d(gen_dim*8, gen_dim*4, 4, 2, 1, bias=False),\n",
    "          nn.BatchNorm2d(gen_dim*4),\n",
    "          nn.ReLU(True),\n",
    "  \n",
    "          nn.ConvTranspose2d(gen_dim*4, gen_dim*2, 4, 2, 1, bias=False),\n",
    "          nn.BatchNorm2d(gen_dim*2),\n",
    "          nn.ReLU(True),\n",
    "          \n",
    "          nn.ConvTranspose2d(gen_dim*2, gen_dim, 4, 2, 1, bias=False),\n",
    "          nn.BatchNorm2d(gen_dim),\n",
    "          nn.ReLU(True),\n",
    "  \n",
    "          nn.ConvTranspose2d(gen_dim, no_of_channels, 4, 2, 1, bias=False),\n",
    "          nn.Tanh() # (-1 und 1) ; Tanh wird häufig verwendet da eine begrenzte Aktivierung es dem Modell ermöglicht,\n",
    "                    # schneller zu lernen. (https://arxiv.org/pdf/1511.06434.pdf S. 3)\n",
    "      )\n",
    "\n",
    "    # Feedforward\n",
    "    def forward(self, input):\n",
    "      output = self.generator(input)\n",
    "      return output\n",
    "\n",
    "#Erstellen des Generators und aufs device schieben\n",
    "gen = Generator().to(device)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Critic/ Diskrimnator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (discriminator): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, no_of_channels=no_of_channels, disc_dim=IMAGE_SIZE):\n",
    "        \"\"\"\n",
    "        Critic 1 Input Layer; 3 Hidden Layer ; 1 Output Layer\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "                # Output = ((inputsize) + 2*padding + (kernelsite-1)-1/stride) -1\n",
    "                nn.Conv2d(no_of_channels, disc_dim, 4, 2, 1, bias=False),  # conv2d hilft dabei aus einem großem Tensor einen kleinen Tensor zu stellen\n",
    "                nn.LeakyReLU(0.2, inplace=True),    # Leaky RELU lässt negative Werte zu (nicht wie RELU); Neuronen werden somit nicht auf Null gesetzt\n",
    "                                                    # Hilft dem Generator, da dieser nur \"Lernen\" kann wenn er vom Diskriminator einen Gradienten erhält\n",
    "                \n",
    "                nn.Conv2d(disc_dim, disc_dim * 2, 4, 2, 1, bias=False),\n",
    "                nn.InstanceNorm2d(disc_dim * 2, affine=True),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                \n",
    "                nn.Conv2d(disc_dim * 2, disc_dim * 4, 3, 2, 1, bias=False),\n",
    "                nn.InstanceNorm2d(disc_dim * 4, affine=True),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "                nn.Conv2d(disc_dim * 4, disc_dim * 8, 3, 2, 1, bias=False),\n",
    "                nn.InstanceNorm2d(disc_dim * 8, affine=True),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                \n",
    "                nn.Conv2d(disc_dim * 8, 1, 4, 1, 0, bias=False),\n",
    "                \n",
    "            )\n",
    "\n",
    "    #Feedforward\n",
    "    def forward(self, input):\n",
    "        output = self.discriminator(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "        #return output\n",
    "\n",
    "# Erstellen des Crtics und verschieben auf das Device\n",
    "critic =Discriminator().to(device)\n",
    "print(critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Torchsummary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary-Generator\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1          [-1, 512, 67, 67]         819,200\n",
      "       BatchNorm2d-2          [-1, 512, 67, 67]           1,024\n",
      "              ReLU-3          [-1, 512, 67, 67]               0\n",
      "   ConvTranspose2d-4        [-1, 256, 134, 134]       2,097,152\n",
      "       BatchNorm2d-5        [-1, 256, 134, 134]             512\n",
      "              ReLU-6        [-1, 256, 134, 134]               0\n",
      "   ConvTranspose2d-7        [-1, 128, 268, 268]         524,288\n",
      "       BatchNorm2d-8        [-1, 128, 268, 268]             256\n",
      "              ReLU-9        [-1, 128, 268, 268]               0\n",
      "  ConvTranspose2d-10         [-1, 64, 536, 536]         131,072\n",
      "      BatchNorm2d-11         [-1, 64, 536, 536]             128\n",
      "             ReLU-12         [-1, 64, 536, 536]               0\n",
      "  ConvTranspose2d-13        [-1, 3, 1072, 1072]           3,072\n",
      "             Tanh-14        [-1, 3, 1072, 1072]               0\n",
      "================================================================\n",
      "Total params: 3,576,704\n",
      "Trainable params: 3,576,704\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.56\n",
      "Forward/backward pass size (MB): 841.69\n",
      "Params size (MB): 13.64\n",
      "Estimated Total Size (MB): 856.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary-Generator\")\n",
    "summary(gen,(LATENT_SIZE,IMAGE_SIZE,IMAGE_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary-Critic/Discriminator\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           3,072\n",
      "         LeakyReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3          [-1, 128, 16, 16]         131,072\n",
      "    InstanceNorm2d-4          [-1, 128, 16, 16]             256\n",
      "         LeakyReLU-5          [-1, 128, 16, 16]               0\n",
      "            Conv2d-6            [-1, 256, 8, 8]         294,912\n",
      "    InstanceNorm2d-7            [-1, 256, 8, 8]             512\n",
      "         LeakyReLU-8            [-1, 256, 8, 8]               0\n",
      "            Conv2d-9            [-1, 512, 4, 4]       1,179,648\n",
      "   InstanceNorm2d-10            [-1, 512, 4, 4]           1,024\n",
      "        LeakyReLU-11            [-1, 512, 4, 4]               0\n",
      "           Conv2d-12              [-1, 1, 1, 1]           8,192\n",
      "================================================================\n",
      "Total params: 1,618,688\n",
      "Trainable params: 1,618,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 2.31\n",
      "Params size (MB): 6.17\n",
      "Estimated Total Size (MB): 8.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary-Critic/Discriminator\")\n",
    "summary(critic,(no_of_channels,IMAGE_SIZE,IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Gewichtsinitialisierung von Generator und Critic/ Diskriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gewichte initialisiert\n"
     ]
    }
   ],
   "source": [
    "# mean 0 and Standardabweichung 0.02\n",
    "def w_initial(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "# Gewichtsinitialisierung von Generator und Critic/ Discriminator\n",
    "gen = gen.apply(w_initial)\n",
    "critic = critic.apply(w_initial)\n",
    "print(\"Gewichte initialisiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=LR, betas=(0, 0.9))\n",
    "critic_opt = torch.optim.Adam(critic.parameters(), lr=LR, betas=(0, 0.9))               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " def gradient_penalty( critic, real_image, fake_image, device=device):\n",
    "    # Übernahme der Batchsize, Channels, Höhe und Breite des realen Images\n",
    "    batch_size, channel, height, width= real_image.shape\n",
    "    \n",
    "    # alpha radomisiert zwischen 0 und 1 gewählt\n",
    "    alpha= torch.rand(batch_size,1,1,1).repeat(1, channel, height, width).to(device)\n",
    "    \n",
    "    # interpoliertes Bild = zufällig gewichteter Durchschnitt zwischen einem realen und einem fake Image\n",
    "    interpolatted_image = (alpha*real_image) + (1-alpha) * fake_image # Alpha *echtes Bild + (1 − Alpha) * gefälschtes Bild\n",
    "    \n",
    "    # Berechnung des critic-scores auf einem interpolierten Bild\n",
    "    interpolated_score= critic(interpolatted_image)\n",
    "    \n",
    "    # Gradient Interpoliertes Bild\n",
    "    gradient= torch.autograd.grad(inputs=interpolatted_image,\n",
    "                                  outputs=interpolated_score,\n",
    "                                  retain_graph=True,\n",
    "                                  create_graph=True,\n",
    "                                  grad_outputs=torch.ones_like(interpolated_score)                          \n",
    "                                 )[0]\n",
    "    gradient = gradient.view(gradient.shape[0],-1)\n",
    "    gradient_norm =  gradient.norm(2,dim=1) # Normalisierung \n",
    "    gradient_penalty = torch.mean((gradient_norm-1)**2) # Mean\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hilfsfunktionen: Normalisierung von Tensoren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_norm(img_tensors):\n",
    "    # print (img_tensors)\n",
    "    # print (img_tensors * NORM [1][0] + NORM [0][0])\n",
    "    return img_tensors * NORM[1][0] + NORM[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Ordner anlegen für die vom Generator erstellten Images, Anzeigen der genierten Images (Fakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordner unter \"../data/\" für die genierten Fake Images anlegen\n",
    "dir_gen_samples = '../data/outputs/'\n",
    "os.makedirs(dir_gen_samples, exist_ok=True)    \n",
    "\n",
    "# Funktion zum Speichern der generierten Bilder    \n",
    "def saves_gen_samples(idx, random_Tensor):\n",
    "\n",
    "    # Randomisierter Tensor wird an den Generator übergeben\n",
    "    fake_img = gen(random_Tensor)\n",
    "\n",
    "    # Setzen von Bildbezeichnungen für die Fake_Images\n",
    "    fake_img_name = \"gen_img-{0:0=4d}.png\".format(idx)\n",
    "\n",
    "    # Tensor-Normalisierung; Speichern der Fake_Images im Ordner \"Outputs/dir_gen_samples/\"\n",
    "    save_image(tensor_norm(fake_img), os.path.join(\n",
    "        dir_gen_samples, fake_img_name), nrow=8)\n",
    "    print(\"Gespeichert\")\n",
    "\n",
    "# Funktion zum anzeigen von Images\n",
    "def display_images(image_tensor, num_images=25, size=(3, 64, 64)):\n",
    "\n",
    "    image = image_tensor.detach().cpu().view(-1, *size) # Images Flatten  \n",
    "    image_grid = make_grid(image[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "570ceabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/994 [01:17<10:58:04, 39.80s/it]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x116c61b80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1301, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "  0%|          | 2/994 [01:44<14:27:01, 52.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1d/rpn2qlds4fd_l_8rtzmvk18m0000gp/T/ipykernel_15604/1064717963.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# Backprop. + Aufzeichnen dynamischen Graphen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Update Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Gen_losses = []\n",
    "Critic_losses = []\n",
    "\n",
    "# Iteration über Epochen    \n",
    "for epoch in range(NUM_EPOCH):\n",
    "    \n",
    "    # Iteration über Batches\n",
    "    for real_image, _ in tqdm(dataloader):\n",
    "        \n",
    "        # Aktuelle Batchsize\n",
    "        cur_batch_size = real_image.shape[0]\n",
    "\n",
    "        # Real Images auf Device\n",
    "        real_image = real_image.to(device)\n",
    "        \n",
    "        #Iteration über Critic (=Discrimiator) Anzahl\n",
    "        for _ in range(N_CRITIC):\n",
    "            \n",
    "            # Generieren von Radom-Noise\n",
    "            fake_noise = get_noise(cur_batch_size, LATENT_SIZE, device=device)\n",
    "            fake = gen(fake_noise)\n",
    "            \n",
    "            # Trainieren des Critics (=Discriminator)\n",
    "            critic_fake_pred = critic(fake).reshape(-1)\n",
    "            critic_real_pred = critic(real_image).reshape(-1)\n",
    "            \n",
    "            # Berechnung: gradient penalty auf den realen and fake Images (Generiert durch Generator)\n",
    "            gp = gradient_penalty(critic, real_image, fake, device)\n",
    "            critic_loss = -(torch.mean(critic_real_pred) -\n",
    "                            torch.mean(critic_fake_pred)) + LAMBDA_GP * gp\n",
    "            \n",
    "            # Gradient = 0 \n",
    "            critic.zero_grad()\n",
    "            \n",
    "            # Backprop. + Aufzeichnen dynamischen Graphen \n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            \n",
    "            # Update Optimizer\n",
    "            critic_opt.step()\n",
    "\n",
    "        # Trainieren des Generators: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        gen_fake = critic(fake).reshape(-1)\n",
    "        gen_loss = -torch.mean(gen_fake)\n",
    "        \n",
    "        # Gradient = 0 \n",
    "        gen.zero_grad()\n",
    "        \n",
    "        # Backprop.\n",
    "        gen_loss.backward()\n",
    "        \n",
    "        # Update optimizer\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Visualisierung nach Anzahl Display_Step (=500)\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            \n",
    "            # Ausgabe des Gen-Loss und Critic-Loss\n",
    "            print(\n",
    "                f\"Step {cur_step}: Generator loss: {gen_loss}, critic loss: {critic_loss}\")\n",
    "\n",
    "            # Speichern des Gesamtlosses von Critic/ Diskriminator und Generator\n",
    "            Critic_losses.append(critic_loss)\n",
    "            Gen_losses.append(gen_loss)\n",
    "            \n",
    "            # Anzeigen der Fake Images\n",
    "            display_images(fake)\n",
    "            \n",
    "            #display_images(real_image)\n",
    "            \n",
    "            # Loss = 0 setzen\n",
    "            gen_loss = 0\n",
    "            critic_loss = 0\n",
    "\n",
    "            #Speichern der Fake Images\n",
    "            saves_gen_samples(cur_step, fake_noise)\n",
    "        cur_step += 1 # cur_step = cur_step+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Grafische Darstellung des Losses von Generator und Critic/ Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Darstellung Loss \n",
    "EPOCH_COUNT_G= range(1,len(Gen_losses)+1) # Anzahl der Epochen vom Gen.\n",
    "EPOCH_COUNT_C= range(1,len(Critic_losses)+1) # Anzahl der Epochen vom Dis.\n",
    "\n",
    "G_losses = [gen.item() for gen in Gen_losses ]\n",
    "C_losses = [critic.item() for critic in Critic_losses ]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"LOSS: Generator und Critic/ Discriminator während dem Training\")\n",
    "plt.plot(EPOCH_COUNT_G, G_losses,\"r-\", label=\"Generator\")\n",
    "plt. plot(EPOCH_COUNT_C,C_losses,\"b-\", label=\"Crtic\")\n",
    "plt.xlabel(\"EPOCH\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbe1a3cfd8a5058f1abdf68edb1ce1f2b89102bbafa6eec62f902d8120c37b9c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38_torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
