{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch.autograd as autograd\n",
    "from random import random, weibullvariate\n",
    "import opendatasets as od\n",
    "import os                 # Dient zum lokalen Speichern des Datasets\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torch.utils.data as DataLoader\n",
    "import torchvision.datasets as ImageFolder\n",
    "import torchvision.transforms as transforms  # Transformieren von Bildern\n",
    "import matplotlib.pyplot as plt  # plotten von Grafen/ Bildern\n",
    "import torch.nn as nn  # Neuronales Netz\n",
    "import torch.optim as optim  # Optimierungs-Algorithmen\n",
    "from torchvision.utils import save_image  # Speichern von Bildern\n",
    "from matplotlib import image\n",
    "from tkinter.tix import IMAGE\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "!pip install opendatasets\n",
    "!pip install torchsummary\n",
    "\n",
    "#import torch.nn.functional as F  # Loss\n",
    "#!pip install opendatasets\n",
    "\n",
    "IMAGE_SIZE = 28  # Größe der Bilder\n",
    "BATCH_SIZE = 128  # Anzahl der Batches\n",
    "WORKERS = 2  # Anzahl der Kerne beim Arbeiten auf der GPU\n",
    "# Normalisierung mit 0.5 Mittelwert und Standardabweichung für alle drei Channels der Bilder\n",
    "NORM = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "\n",
    "data_dir = '../data/'\n",
    "os.makedirs(data_dir, exist_ok=True)  # Anlegen eines Ordners für Bilder\n",
    "\n",
    "# Erklärung zum Umgang mit Opendata und Kaggle - https://pypi.org/project/opendatasets/\n",
    "# Datensatz:Anime-Faces werden von Kaggle geladen\n",
    "# Hierfür wird der User-API-KEY benötigt\n",
    "# APIKEY {\"username\":\"kimmhl\",\"key\":\"f585163b4ee30f0a5b44b1a902dc56e6\"}\n",
    "dataset_url = 'https://www.kaggle.com/splcher/animefacedataset'\n",
    "# Images werden in './animefacedataset' gespeichert\n",
    "od.download(dataset_url, data_dir)\n",
    "\n",
    "\"\"\"\n",
    "Ausgabe zu den Ordnern\n",
    "\"\"\"\n",
    "print(os.listdir(data_dir))  # zeigt Ordner an\n",
    "\n",
    "# gibt 10 Bezeichnungen von Bildern aus\n",
    "print(os.listdir(data_dir+'animefacedataset/images')[:10])\n",
    "\n",
    "\n",
    "# Transformer\n",
    "transform = transforms.Compose([\n",
    "    # Resize der Images auf 64 der kürzesten Seite; Andere Seite wird\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    # skaliert, um das Seitenverhältnis des Bildes beizubehalten.\n",
    "    # Zuschneiden auf die Mitte des Images, sodass ein quadratisches Bild mit 64 x 64 Pixeln entsteht\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    # Umwandeln in einen Tensor (Bildern in numerische Werte umwandeln)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*NORM)])          # Normalisierung Mean & Standardabweichung von 0.5 für alle Channels\n",
    "# (Anzahl: 3 für farbige Bilder)\n",
    "# Pixelwerte liegen damit zwischen (-1;1)\n",
    "\n",
    "# Dataset\n",
    "\"\"\"\n",
    "ImageFolder() : Befehl erwartet, dass nach Images nach labeln organisiert sind (root/label/picture.png)\n",
    "\"\"\"\n",
    "org_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=data_dir, transform=transform)\n",
    "\n",
    "# Dataloader\n",
    "\"\"\"\n",
    "Dataloader():\n",
    "\"\"\"\n",
    "org_loader = t.utils.data.DataLoader(org_dataset,              # Dataset (Images)\n",
    "                                     # Es wird auf Batches trainiert, damit auf Basis eines Batch-Fehlers das NN angepasst wird\n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=WORKERS)\n",
    "\n",
    "# Nutzen der GPU wenn vorhanden, ansonsten CPU\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    if t.cuda.is_available():     # Wenn cuda verfügbar dann:\n",
    "        return t.device('cuda')   # Nutze Device = Cuda (=GPU)\n",
    "    else:                         # Ansonsten\n",
    "        return t.device('cpu')    # Nutze Device = CPU\n",
    "\n",
    "\n",
    "# Anzeigen welches Device verfügbar ist\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "# Hilfsklasse zum Verschieben des Dataloaders \"org_loader\" auf das jeweilige Device\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "\n",
    "    # Initialisierung\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    # Anzahl der Images pro Batch\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    # Erstellt einen Batch an Tensoren nach dem Verschieben auf das Device\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            yield tuple(tensor.to(self.device) for tensor in batch)\n",
    "\n",
    "\n",
    "# Dataloader auf dem verfügbaren Device\n",
    "dataloader = DeviceDataLoader(org_loader, device)\n",
    "\n",
    "\n",
    "def get_noise(n_samples, noise_dim, device=device):\n",
    "    '''\n",
    "    Generate noise vectors from the random normal distribution with dimensions (n_samples, noise_dim),\n",
    "    where\n",
    "        n_samples: the number of samples to generate based on  batch_size\n",
    "        noise_dim: the dimension of the noise vector\n",
    "        device: device type can be cuda or cpu\n",
    "    '''\n",
    "\n",
    "    return torch.randn(n_samples, noise_dim, 1, 1, device=device)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, no_of_channels=3, noise_dim=100, gen_dim=64):\n",
    "      super(Generator, self).__init__()\n",
    "      self.network = nn.Sequential(\n",
    "          nn.ConvTranspose2d(noise_dim, gen_dim*4, 4, 1, 0, bias=False),\n",
    "          nn.BatchNorm2d(gen_dim*4),\n",
    "          nn.ReLU(True),\n",
    "\n",
    "          nn.ConvTranspose2d(gen_dim*4, gen_dim*2, 3, 2, 1, bias=False),\n",
    "          nn.BatchNorm2d(gen_dim*2),\n",
    "          nn.ReLU(True),\n",
    "\n",
    "          nn.ConvTranspose2d(gen_dim*2, gen_dim, 4, 2, 1, bias=False),\n",
    "          nn.BatchNorm2d(gen_dim),\n",
    "          nn.ReLU(True),\n",
    "\n",
    "          nn.ConvTranspose2d(gen_dim, no_of_channels, 4, 2, 1, bias=False),\n",
    "          nn.Tanh()\n",
    "      )\n",
    "\n",
    "    def forward(self, input):\n",
    "      output = self.network(input)\n",
    "      return output\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, no_of_channels=3, disc_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(no_of_channels, disc_dim, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(disc_dim, disc_dim * 2, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(disc_dim * 2, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(disc_dim * 2, disc_dim * 4, 3, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(disc_dim * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(disc_dim * 4, 1, 4, 1, 0, bias=False),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.network(input)\n",
    "        #return output.view(-1, 1).squeeze(1)\n",
    "        return output\n",
    "\n",
    "\n",
    "gen = Generator().to(device)\n",
    "critic = Discriminator().to(device)\n",
    "\n",
    "# You initialize the weights to the normal distribution\n",
    "# with mean 0 and standard deviation 0.02\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "\n",
    "gen = gen.apply(weights_init)\n",
    "critic = critic.apply(weights_init)\n",
    "\n",
    "lr = 1e-4\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "gen_opt = torch.optim.RMSprop(gen.parameters(), lr=lr)\n",
    "critic_opt = torch.optim.RMSprop(critic.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def gradient_penalty(critic, real_image, fake_image, device=device):\n",
    "    batch_size, channel, height, width = real_image.shape\n",
    "    #alpha is selected randomly between 0 and 1\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1).repeat(\n",
    "        1, channel, height, width).to(device)\n",
    "    # interpolated image=randomly weighted average between a real and fake image\n",
    "    #interpolated image ← alpha *real image  + (1 − alpha) fake image\n",
    "    interpolatted_image = (alpha*real_image) + (1-alpha) * fake_image\n",
    "\n",
    "    # calculate the critic score on the interpolated image\n",
    "    interpolated_score = critic(interpolatted_image)\n",
    "\n",
    "    # take the gradient of the score wrt to the interpolated image\n",
    "    gradient = torch.autograd.grad(inputs=interpolatted_image,\n",
    "                                   outputs=interpolated_score,\n",
    "                                   retain_graph=True,\n",
    "                                   create_graph=True,\n",
    "                                   grad_outputs=torch.ones_like(\n",
    "                                       interpolated_score)\n",
    "                                   )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm-1)**2)\n",
    "    return gradient_penalty\n",
    "\n",
    "# Hilfsfunktionen zur Normalisierng von Tensoren und grafischen Darstellung\n",
    "\n",
    "\n",
    "def tensor_norm(img_tensors):\n",
    "    # print (img_tensors)\n",
    "    # print (img_tensors * NORM [1][0] + NORM [0][0])\n",
    "    return img_tensors * NORM[1][0] + NORM[0][0]\n",
    "\n",
    "\n",
    "def show_images(images, nmax=64):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.title(\"Fake_Images\")\n",
    "    ax.imshow(make_grid(tensor_norm(images.detach()[:nmax]), nrow=8).permute(\n",
    "        1, 2, 0).cpu())  # detach() : erstellt eine neue \"Ansicht\",\n",
    "    # sodass diese Operationen nicht mehr verfolgt werden,orm\n",
    "    # d. h. der Gradient wird nicht berechnet und der Untergraph\n",
    "    # wird nicht aufgezeichnet > Speicher wird nicht verwendet\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Ordner anlegen für die vom Generator erstellten Images\n",
    "\"\"\"\n",
    "\n",
    "dir_gen_samples = '../data/outputs/'\n",
    "#os.makedirs('../outputs/dir_gen_samples', exist_ok=True)\n",
    "os.makedirs(dir_gen_samples, exist_ok=True)\n",
    "\n",
    "\n",
    "def saves_gen_samples(idx, random_Tensor):\n",
    "    # Randomisierter Tensor wird an den Generator übergeben\n",
    "    fake_img = gen(random_Tensor)\n",
    "    # Setzen von Bildbezeichnungen für die Fake_Images\n",
    "    fake_img_name = \"gen_img-{0:0=4d}.png\".format(idx)\n",
    "    # Tensor-Normalisierung; Speichern der Fake_Images im Ordner \"Outputs/dir_gen_samples/\"\n",
    "    save_image(tensor_norm(fake_img), os.path.join(\n",
    "        dir_gen_samples, fake_img_name), nrow=8)\n",
    "    show_images(fake_img)  # Plotten der Fake_Images\n",
    "    print(\"Gespeichert\")\n",
    "\n",
    "\n",
    "def display_images(image_tensor, num_images=25, size=(3, 28, 28)):\n",
    "\n",
    "    flatten_image = image_tensor.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(flatten_image[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Training\n",
    "\n",
    "n_epochs = 3\n",
    "cur_step = 0\n",
    "LAMBDA_GP = 10\n",
    "display_step = 500\n",
    "z_dim = 100\n",
    "CRITIC_ITERATIONS = 5\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "start_idx = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches\n",
    "    for real_image, _ in tqdm(dataloader):\n",
    "        cur_batch_size = real_image.shape[0]\n",
    "\n",
    "        real_image = real_image.to(device)\n",
    "\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "            fake = gen(fake_noise)\n",
    "            critic_fake_pred = critic(fake).reshape(-1)\n",
    "            critic_real_pred = critic(real_image).reshape(-1)\n",
    "            #Calculate gradient penalty on real and fake images generated by generator\n",
    "            gp = gradient_penalty(critic, real_image, fake, device)\n",
    "            critic_loss = -(torch.mean(critic_real_pred) -\n",
    "                            torch.mean(critic_fake_pred)) + LAMBDA_GP * gp\n",
    "\n",
    "            critic.zero_grad()\n",
    "            #To make a backward pass and retain the intermediary results\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            critic_opt.step()\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        gen_fake = critic(fake).reshape(-1)\n",
    "        gen_loss = -torch.mean(gen_fake)\n",
    "\n",
    "        #print( \"D_loss:\", critic_loss, \"G_Loss:\", gen_loss)\n",
    "\n",
    "        gen.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        # Update optimizer\n",
    "        gen_opt.step()\n",
    "\n",
    "        ## Visualization code ##\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            print(\n",
    "                f\"Step {cur_step}: Generator loss: {gen_loss}, critic loss: {critic_loss}\")\n",
    "            display_images(fake)\n",
    "            display_images(real_image)\n",
    "            gen_loss = 0\n",
    "            critic_loss = 0\n",
    "        cur_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opendatasets\n",
    "!pip install torchsummary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from tkinter.tix import IMAGE\n",
    "from matplotlib import image\n",
    "#import torch.nn.functional as F  # Loss\n",
    "from torchvision.utils import save_image  # Speichern von Bildern\n",
    "import torch.optim as optim  # Optimierungs-Algorithmen\n",
    "import torch.nn as nn  # Neuronales Netz\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt  # plotten von Grafen/ Bildern\n",
    "import torchvision.transforms as transforms  # Transformieren von Bildern\n",
    "import torchvision.datasets as ImageFolder\n",
    "import torch.utils.data as DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import os                 # Dient zum lokalen Speichern des Datasets\n",
    "import opendatasets as od\n",
    "from random import random, weibullvariate\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "from torchsummary import summary\n",
    "#!pip install opendatasets\n",
    "\n",
    "IMAGE_SIZE = 64  # Größe der Bilder\n",
    "BATCH_SIZE = 128  # Anzahl der Batches\n",
    "WORKERS = 2  # Anzahl der Kerne beim Arbeiten auf der GPU\n",
    "# Normalisierung mit 0.5 Mittelwert und Standardabweichung für alle drei Channels der Bilder\n",
    "NORM = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "\n",
    "data_dir = './data/'\n",
    "os.makedirs(data_dir, exist_ok=True)  # Anlegen eines Ordners für Bilder\n",
    "\n",
    "# Erklärung zum Umgang mit Opendata und Kaggle - https://pypi.org/project/opendatasets/\n",
    "# Datensatz:Anime-Faces werden von Kaggle geladen\n",
    "# Hierfür wird der User-API-KEY benötigt\n",
    "# APIKEY {\"username\":\"kimmhl\",\"key\":\"f585163b4ee30f0a5b44b1a902dc56e6\"}\n",
    "dataset_url = 'https://www.kaggle.com/splcher/animefacedataset'\n",
    "# Images werden in './animefacedataset' gespeichert\n",
    "od.download(dataset_url, data_dir)\n",
    "\n",
    "\"\"\"\n",
    "Ausgabe zu den Ordnern\n",
    "\"\"\"\n",
    "print(os.listdir(data_dir))  # zeigt Ordner an\n",
    "\n",
    "# gibt 10 Bezeichnungen von Bildern aus\n",
    "print(os.listdir(data_dir+'animefacedataset/images')[:10])\n",
    "\n",
    "\n",
    "# Transformer\n",
    "transform = transforms.Compose([\n",
    "    # Resize der Images auf 64 der kürzesten Seite; Andere Seite wird\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    # skaliert, um das Seitenverhältnis des Bildes beizubehalten.\n",
    "    # Zuschneiden auf die Mitte des Images, sodass ein quadratisches Bild mit 64 x 64 Pixeln entsteht\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    # Umwandeln in einen Tensor (Bildern in numerische Werte umwandeln)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*NORM)])          # Normalisierung Mean & Standardabweichung von 0.5 für alle Channels\n",
    "# (Anzahl: 3 für farbige Bilder)\n",
    "# Pixelwerte liegen damit zwischen (-1;1)\n",
    "\n",
    "# Dataset\n",
    "\"\"\"\n",
    "ImageFolder() : Befehl erwartet, dass nach Images nach labeln organisiert sind (root/label/picture.png)\n",
    "\"\"\"\n",
    "org_dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Dataloader\n",
    "\"\"\"\n",
    "Dataloader():\n",
    "\"\"\"\n",
    "org_loader = t.utils.data.DataLoader(org_dataset,              # Dataset (Images)\n",
    "                                     # Es wird auf Batches trainiert, damit auf Basis eines Batch-Fehlers das NN angepasst wird\n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=WORKERS)\n",
    "\n",
    "# Nutzen der GPU wenn vorhanden, ansonsten CPU\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    if t.cuda.is_available():     # Wenn cuda verfügbar dann:\n",
    "        return t.device('cuda')   # Nutze Device = Cuda (=GPU)\n",
    "    else:                         # Ansonsten\n",
    "        return t.device('cpu')    # Nutze Device = CPU\n",
    "\n",
    "\n",
    "# Anzeigen welches Device verfügbar ist\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "# Hilfsklasse zum Verschieben des Dataloaders \"org_loader\" auf das jeweilige Device\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "\n",
    "    # Initialisierung\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    # Anzahl der Images pro Batch\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    # Erstellt einen Batch an Tensoren nach dem Verschieben auf das Device\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            yield tuple(tensor.to(self.device) for tensor in batch)\n",
    "\n",
    "\n",
    "# Dataloader auf dem verfügbaren Device\n",
    "dataloader = DeviceDataLoader(org_loader, device)\n",
    "\n",
    "                      \n",
    "def get_noise(n_samples, noise_dim, device=device):\n",
    "    '''\n",
    "    Generate noise vectors from the random normal distribution with dimensions (n_samples, noise_dim),\n",
    "    where\n",
    "        n_samples: the number of samples to generate based on  batch_size\n",
    "        noise_dim: the dimension of the noise vector\n",
    "        device: device type can be cuda or cpu\n",
    "    '''\n",
    "    \n",
    "    return  torch.randn(n_samples,noise_dim, 1,1,device=device)\n",
    "\n",
    "                      \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, no_of_channels=3, noise_dim=100, gen_dim=64):\n",
    "      super(Generator, self).__init__()\n",
    "      self.network = nn.Sequential(\n",
    "          nn.ConvTranspose2d(noise_dim, gen_dim*8, 4, 1, 0, bias=False),\n",
    "          nn.BatchNorm2d(gen_dim*8),\n",
    "          nn.ReLU(True),\n",
    "  \n",
    "          nn.ConvTranspose2d(gen_dim*8, gen_dim*4, 4, 2, 1, bias=False),\n",
    "          nn.BatchNorm2d(gen_dim*4),\n",
    "          nn.ReLU(True),\n",
    "  \n",
    "          nn.ConvTranspose2d(gen_dim*4, gen_dim*2, 4, 2, 1, bias=False),\n",
    "          nn.BatchNorm2d(gen_dim*2),\n",
    "          nn.ReLU(True),\n",
    "          \n",
    "          nn.ConvTranspose2d(gen_dim*2, gen_dim, 4, 2, 1, bias=False),\n",
    "          nn.BatchNorm2d(gen_dim),\n",
    "          nn.ReLU(True),\n",
    "  \n",
    "          nn.ConvTranspose2d(gen_dim, no_of_channels, 4, 2, 1, bias=False),\n",
    "          nn.Tanh()\n",
    "      )\n",
    "  \n",
    "    def forward(self, input):\n",
    "      output = self.network(input)\n",
    "      return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, no_of_channels=3, disc_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "                \n",
    "                nn.Conv2d(no_of_channels, disc_dim, 4, 2, 1, bias=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                \n",
    "                nn.Conv2d(disc_dim, disc_dim * 2, 4, 2, 1, bias=False),\n",
    "                nn.InstanceNorm2d(disc_dim * 2, affine=True),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                \n",
    "                nn.Conv2d(disc_dim * 2, disc_dim * 4, 3, 2, 1, bias=False),\n",
    "                nn.InstanceNorm2d(disc_dim * 4, affine=True),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "                nn.Conv2d(disc_dim * 4, disc_dim * 8, 3, 2, 1, bias=False),\n",
    "                nn.InstanceNorm2d(disc_dim * 8, affine=True),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                \n",
    "                nn.Conv2d(disc_dim * 8, 1, 4, 1, 0, bias=False),\n",
    "                \n",
    "            )\n",
    "    def forward(self, input):\n",
    "        output = self.network(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "        #return output\n",
    "                      \n",
    "gen = Generator().to(device)\n",
    "critic =Discriminator().to(device)\n",
    "                      \n",
    "# You initialize the weights to the normal distribution                      \n",
    "# with mean 0 and standard deviation 0.02\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(m.bias, val=0)\n",
    "gen = gen.apply(weights_init)\n",
    "critic = critic.apply(weights_init)\n",
    "                      \n",
    "lr = 1e-4\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "gen_opt = torch.optim.RMSprop(gen.parameters(), lr=lr)\n",
    "critic_opt = torch.optim.RMSprop(critic.parameters(), lr=lr)\n",
    "                      \n",
    "                      \n",
    "                      \n",
    "def gradient_penalty( critic, real_image, fake_image, device=device):\n",
    "    batch_size, channel, height, width= real_image.shape\n",
    "    #alpha is selected randomly between 0 and 1\n",
    "    alpha= torch.rand(batch_size,1,1,1).repeat(1, channel, height, width).to(device)\n",
    "    # interpolated image=randomly weighted average between a real and fake image\n",
    "    #interpolated image ← alpha *real image  + (1 − alpha) fake image\n",
    "    interpolatted_image=(alpha*real_image) + (1-alpha) * fake_image\n",
    "    \n",
    "    # calculate the critic score on the interpolated image\n",
    "    interpolated_score= critic(interpolatted_image)\n",
    "    \n",
    "    # take the gradient of the score wrt to the interpolated image\n",
    "    gradient= torch.autograd.grad(inputs=interpolatted_image,\n",
    "                                  outputs=interpolated_score,\n",
    "                                  retain_graph=True,\n",
    "                                  create_graph=True,\n",
    "                                  grad_outputs=torch.ones_like(interpolated_score)                          \n",
    "                                 )[0]\n",
    "    gradient= gradient.view(gradient.shape[0],-1)\n",
    "    gradient_norm= gradient.norm(2,dim=1)\n",
    "    gradient_penalty=torch.mean((gradient_norm-1)**2)\n",
    "    return gradient_penalty\n",
    "\n",
    "# Hilfsfunktionen zur Normalisierng von Tensoren und grafischen Darstellung\n",
    "def tensor_norm(img_tensors):\n",
    "    # print (img_tensors)\n",
    "    # print (img_tensors * NORM [1][0] + NORM [0][0])\n",
    "    return img_tensors * NORM[1][0] + NORM[0][0]\n",
    "\n",
    "def show_images(images, nmax=64):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.title(\"Fake_Images\")\n",
    "    ax.imshow(make_grid(tensor_norm(images.detach()[:nmax]), nrow=8).permute(\n",
    "        1, 2, 0).cpu())  # detach() : erstellt eine neue \"Ansicht\",\n",
    "    # sodass diese Operationen nicht mehr verfolgt werden,orm\n",
    "    # d. h. der Gradient wird nicht berechnet und der Untergraph\n",
    "    # wird nicht aufgezeichnet > Speicher wird nicht verwendet\n",
    "\n",
    "\"\"\"\n",
    "Ordner anlegen für die vom Generator erstellten Images\n",
    "\"\"\"\n",
    "\n",
    "dir_gen_samples = '../data/outputs/'\n",
    "#os.makedirs('../outputs/dir_gen_samples', exist_ok=True)\n",
    "os.makedirs(dir_gen_samples, exist_ok=True)    \n",
    "    \n",
    "def saves_gen_samples(idx, random_Tensor):\n",
    "    # Randomisierter Tensor wird an den Generator übergeben\n",
    "    fake_img = gen(random_Tensor)\n",
    "    # Setzen von Bildbezeichnungen für die Fake_Images\n",
    "    fake_img_name = \"gen_img-{0:0=4d}.png\".format(idx)\n",
    "    # Tensor-Normalisierung; Speichern der Fake_Images im Ordner \"Outputs/dir_gen_samples/\"\n",
    "    save_image(tensor_norm(fake_img), os.path.join(\n",
    "        dir_gen_samples, fake_img_name), nrow=8)\n",
    "    show_images(fake_img)  # Plotten der Fake_Images\n",
    "    print(\"Gespeichert\")\n",
    "\n",
    "\n",
    "def display_images(image_tensor, num_images=25, size=(3, 64, 64)):\n",
    "    \n",
    "    flatten_image = image_tensor.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(flatten_image[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Training \n",
    "                      \n",
    "n_epochs = 3\n",
    "cur_step = 0\n",
    "LAMBDA_GP = 10\n",
    "display_step = 500\n",
    "z_dim=100\n",
    "CRITIC_ITERATIONS = 5\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "start_idx = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches\n",
    "    for real_image, _ in tqdm(dataloader):\n",
    "        cur_batch_size = real_image.shape[0]\n",
    "    \n",
    "        real_image = real_image.to(device)\n",
    "    \n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "            fake = gen(fake_noise)\n",
    "            critic_fake_pred = critic(fake).reshape(-1)\n",
    "            critic_real_pred = critic(real_image).reshape(-1)\n",
    "            #Calculate gradient penalty on real and fake images generated by generator\n",
    "            gp=gradient_penalty(critic, real_image, fake, device)\n",
    "            critic_loss = -(torch.mean(critic_real_pred) -torch.mean(critic_fake_pred)) + LAMBDA_GP *gp\n",
    "            \n",
    "            critic.zero_grad()\n",
    "            #To make a backward pass and retain the intermediary results\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            critic_opt.step()\n",
    "            \n",
    "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        gen_fake= critic(fake).reshape(-1)\n",
    "        gen_loss = -torch.mean(gen_fake)\n",
    "        \n",
    "        #print( \"D_loss:\", critic_loss, \"G_Loss:\", gen_loss)\n",
    "\n",
    "        gen.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        # Update optimizer\n",
    "        gen_opt.step()\n",
    "        \n",
    "        ## Visualization code ##\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            print(f\"Step {cur_step}: Generator loss: {gen_loss}, critic loss: {critic_loss}\")\n",
    "            display_images(fake)\n",
    "            display_images(real_image)\n",
    "            gen_loss = 0\n",
    "            critic_loss = 0\n",
    "        cur_step += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (0.1.20)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from opendatasets) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from opendatasets) (4.62.3)\n",
      "Requirement already satisfied: kaggle in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from opendatasets) (1.5.12)\n",
      "Requirement already satisfied: urllib3 in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (1.26.7)\n",
      "Requirement already satisfied: python-slugify in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (5.0.2)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (2021.10.8)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (2.26.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.10 in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from kaggle->opendatasets) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from requests->kaggle->opendatasets) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages (from requests->kaggle->opendatasets) (3.1)\n",
      "Skipping, found downloaded files in \"./data/animefacedataset\" (use force=True to force download)\n",
      "['animefacedataset']\n",
      "['4426_2003.jpg', '38921_2012.jpg', '55591_2016.jpg', '8777_2004.jpg', '56274_2017.jpg', '24208_2008.jpg', '13759_2006.jpg', '19302_2007.jpg', '14698_2006.jpg', '30569_2010.jpg']\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 42/994 [17:45<6:42:21, 25.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1d/rpn2qlds4fd_l_8rtzmvk18m0000gp/T/ipykernel_6650/622989572.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Backprop. + Aufzeichnen dynamischen Graphen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# Update Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/py38_torch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!pip install opendatasets\n",
    "\n",
    "\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from random import random, weibullvariate\n",
    "import opendatasets as od\n",
    "import os                 # Dient zum lokalen Speichern des Datasets\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torch.utils.data as DataLoader\n",
    "import torchvision.datasets as ImageFolder\n",
    "import torchvision.transforms as transforms  # Transformieren von Bildern\n",
    "import matplotlib.pyplot as plt  # plotten von Grafen/ Bildern\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn as nn  # Neuronales Netz\n",
    "import torch.optim as optim  # Optimierungs-Algorithmen\n",
    "from torchvision.utils import save_image  # Speichern von Bildern\n",
    "from matplotlib import image\n",
    "from tkinter.tix import IMAGE\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "IMAGE_SIZE = 64  # Größe der Bilder\n",
    "BATCH_SIZE = 64  # Anzahl der Batches\n",
    "WORKERS = 2  # Anzahl der Kerne beim Arbeiten auf der GPU\n",
    "# Normalisierung mit 0.5 Mittelwert und Standardabweichung für alle drei Channels der Bilder\n",
    "NORM = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "WORKERS = 2  # Anzahl der Kerne beim Arbeiten auf der GPU\n",
    "NUM_EPOCH = 4  # Anzahl der Epochen\n",
    "LR = 1e-4  # Learningrate\n",
    "LATENT_SIZE = 100  # Radom Input für den Generator\n",
    "N_CRITIC = 5\n",
    "LAMBDA_GP = 10  # Penalty Koeffizient\n",
    "no_of_channels = 3\n",
    "cur_step = 0\n",
    "display_step = 500\n",
    "\n",
    "data_dir = './data/'\n",
    "os.makedirs(data_dir, exist_ok=True)  # Anlegen eines Ordners für Bilder\n",
    "\n",
    "# Erklärung zum Umgang mit Opendata und Kaggle - https://pypi.org/project/opendatasets/\n",
    "# Datensatz:Anime-Faces werden von Kaggle geladen\n",
    "# Hierfür wird der User-API-KEY benötigt\n",
    "# APIKEY {\"username\":\"kimmhl\",\"key\":\"f585163b4ee30f0a5b44b1a902dc56e6\"}\n",
    "dataset_url = 'https://www.kaggle.com/splcher/animefacedataset'\n",
    "# Images werden in './animefacedataset' gespeichert\n",
    "od.download(dataset_url, data_dir)\n",
    "\n",
    "\"\"\"\n",
    "Ausgabe zu den Ordnern\n",
    "\"\"\"\n",
    "print(os.listdir(data_dir))  # zeigt Ordner an)\n",
    "\n",
    "# gibt 10 Bezeichnungen von Bildern aus\n",
    "print(os.listdir(data_dir+'animefacedataset/images')[:10])\n",
    "\n",
    "\n",
    "# Transformer\n",
    "transform = transforms.Compose([\n",
    "    # Resize der Images auf 64 der kürzesten Seite; Andere Seite wird\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    # skaliert, um das Seitenverhältnis des Bildes beizubehalten.\n",
    "    # Zuschneiden auf die Mitte des Images, sodass ein quadratisches Bild mit 64 x 64 Pixeln entsteht\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    # Umwandeln in einen Tensor (Bildern in numerische Werte umwandeln)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*NORM)])          # Normalisierung Mean & Standardabweichung von 0.5 für alle Channels\n",
    "# (Anzahl: 3 für farbige Bilder)\n",
    "# Pixelwerte liegen damit zwischen (-1;1)\n",
    "\n",
    "# Dataset\n",
    "\"\"\"\n",
    "ImageFolder() : Befehl erwartet, dass nach Images nach labeln organisiert sind (root/label/picture.png)\n",
    "\"\"\"\n",
    "org_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=data_dir, transform=transform)\n",
    "\n",
    "# Dataloader\n",
    "\"\"\"\n",
    "Dataloader():\n",
    "\"\"\"\n",
    "org_loader = t.utils.data.DataLoader(org_dataset,              # Dataset (Images)\n",
    "                                     # Es wird auf Batches trainiert, damit auf Basis eines Batch-Fehlers das NN angepasst wird\n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=WORKERS)\n",
    "\n",
    "# Nutzen der GPU wenn vorhanden, ansonsten CPU\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    if t.cuda.is_available():     # Wenn cuda verfügbar dann:\n",
    "        return t.device('cuda')   # Nutze Device = Cuda (=GPU)\n",
    "    else:                         # Ansonsten\n",
    "        return t.device('cpu')    # Nutze Device = CPU\n",
    "\n",
    "\n",
    "# Anzeigen welches Device verfügbar ist\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "# Hilfsklasse zum Verschieben des Dataloaders \"org_loader\" auf das jeweilige Device\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "\n",
    "    # Initialisierung\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    # Anzahl der Images pro Batch\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    # Erstellt einen Batch an Tensoren nach dem Verschieben auf das Device\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            yield tuple(tensor.to(self.device) for tensor in batch)\n",
    "\n",
    "\n",
    "# Dataloader auf dem verfügbaren Device\n",
    "dataloader = DeviceDataLoader(org_loader, device)\n",
    "\n",
    "\n",
    "def get_noise(n_samples, noise_dim, device=device):\n",
    "    '''\n",
    "    Generate noise vectors from the random normal distribution with dimensions (n_samples, noise_dim),\n",
    "    where\n",
    "        n_samples: the number of samples to generate based on batch_size\n",
    "        noise_dim: the dimension of the noise vector\n",
    "        device: device type can be cuda or cpu\n",
    "    '''\n",
    "\n",
    "    return torch.randn(n_samples, noise_dim, 1, 1, device=device)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, no_of_channels=no_of_channels, noise_dim=LATENT_SIZE, gen_dim=IMAGE_SIZE):\n",
    "        super(Generator, self).__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.ConvTranspose2d(noise_dim, gen_dim*8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(gen_dim*8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(gen_dim*8, gen_dim*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(gen_dim*4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(gen_dim*4, gen_dim*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(gen_dim*2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(gen_dim*2, gen_dim, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(gen_dim),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(gen_dim, no_of_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.generator(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, no_of_channels=no_of_channels, disc_dim=IMAGE_SIZE):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(no_of_channels, disc_dim, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(disc_dim, disc_dim * 2, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(disc_dim * 2, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(disc_dim * 2, disc_dim * 4, 3, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(disc_dim * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(disc_dim * 4, disc_dim * 8, 3, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(disc_dim * 8, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(disc_dim * 8, 1, 4, 1, 0, bias=False),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.discriminator(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "        # return output\n",
    "\n",
    "\n",
    "gen = Generator().to(device)\n",
    "critic = Discriminator().to(device)\n",
    "\n",
    "# Gewichtsinitialisierung\n",
    "#  mean 0 and Standardabweichung 0.02\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "\n",
    "gen = gen.apply(weights_init)\n",
    "critic = critic.apply(weights_init)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=LR, betas=(0, 0.9))\n",
    "critic_opt = torch.optim.Adam(critic.parameters(), lr=LR, betas=(0, 0.9))\n",
    "\n",
    "# Gradient Penalty\n",
    "\n",
    "\n",
    "def gradient_penalty(critic, real_image, fake_image, device=device):\n",
    "    batch_size, channel, height, width = real_image.shape\n",
    "    # alpha is selected randomly between 0 and 1\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1).repeat(\n",
    "        1, channel, height, width).to(device)\n",
    "    # interpolated image=randomly weighted average between a real and fake image\n",
    "    # interpolated image ← alpha *real image  + (1 − alpha) fake image\n",
    "    interpolatted_image = (alpha*real_image) + (1-alpha) * fake_image\n",
    "\n",
    "    # calculate the critic score on the interpolated image\n",
    "    interpolated_score = critic(interpolatted_image)\n",
    "\n",
    "    # take the gradient of the score wrt to the interpolated image\n",
    "    gradient = torch.autograd.grad(inputs=interpolatted_image,\n",
    "                                   outputs=interpolated_score,\n",
    "                                   retain_graph=True,\n",
    "                                   create_graph=True,\n",
    "                                   grad_outputs=torch.ones_like(\n",
    "                                       interpolated_score)\n",
    "                                   )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm-1)**2)\n",
    "    return gradient_penalty\n",
    "\n",
    "# Hilfsfunktionen zur Normalisierng von Tensoren und grafischen Darstellung\n",
    "\n",
    "\n",
    "def tensor_norm(img_tensors):\n",
    "    # print (img_tensors)\n",
    "    # print (img_tensors * NORM [1][0] + NORM [0][0])\n",
    "    return img_tensors * NORM[1][0] + NORM[0][0]\n",
    "\n",
    "\n",
    "def show_images(images, nmax=64):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.title(\"Fake_Images\")\n",
    "    ax.imshow(make_grid(tensor_norm(images.detach()[:nmax]), nrow=8).permute(\n",
    "        1, 2, 0).cpu())  # detach() : erstellt eine neue \"Ansicht\",\n",
    "    # sodass diese Operationen nicht mehr verfolgt werden,orm\n",
    "    # d. h. der Gradient wird nicht berechnet und der Untergraph\n",
    "    # wird nicht aufgezeichnet > Speicher wird nicht verwendet\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Ordner anlegen für die vom Generator erstellten Images\n",
    "\"\"\"\n",
    "\n",
    "dir_gen_samples = '../data/outputs/'\n",
    "#os.makedirs('../outputs/dir_gen_samples', exist_ok=True)\n",
    "os.makedirs(dir_gen_samples, exist_ok=True)\n",
    "\n",
    "\n",
    "def saves_gen_samples(idx, random_Tensor):\n",
    "    # Randomisierter Tensor wird an den Generator übergeben\n",
    "    fake_img = gen(random_Tensor)\n",
    "    # Setzen von Bildbezeichnungen für die Fake_Images\n",
    "    fake_img_name = \"gen_img-{0:0=4d}.png\".format(idx)\n",
    "    # Tensor-Normalisierung; Speichern der Fake_Images im Ordner \"Outputs/dir_gen_samples/\"\n",
    "    save_image(tensor_norm(fake_img), os.path.join(\n",
    "        dir_gen_samples, fake_img_name), nrow=8)\n",
    "    # show_images(fake_img)  # Plotten der Fake_Images\n",
    "    print(\"Gespeichert\")\n",
    "\n",
    "\n",
    "def display_images(image_tensor, num_images=25, size=(3, 64, 64)):\n",
    "\n",
    "    flatten_image = image_tensor.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(flatten_image[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Iteration über Epochen\n",
    "for epoch in range(NUM_EPOCH):\n",
    "\n",
    "    # Iteration über Batches\n",
    "    for real_image, _ in tqdm(dataloader):\n",
    "        cur_batch_size = real_image.shape[0]\n",
    "\n",
    "        real_image = real_image.to(device)\n",
    "\n",
    "        # Iteration über Critic (=Discrimiator) Anzahl\n",
    "        for _ in range(N_CRITIC):\n",
    "\n",
    "            # Generieren von Radom-Noise\n",
    "            fake_noise = get_noise(cur_batch_size, LATENT_SIZE, device=device)\n",
    "            fake = gen(fake_noise)\n",
    "\n",
    "            # Trainieren des Critics (=Discriminator)\n",
    "            critic_fake_pred = critic(fake).reshape(-1)\n",
    "            critic_real_pred = critic(real_image).reshape(-1)\n",
    "\n",
    "            # Berechnung: gradient penalty auf den realen and fake Images (Generiert durch Generator)\n",
    "            gp = gradient_penalty(critic, real_image, fake, device)\n",
    "            critic_loss = -(torch.mean(critic_real_pred) -\n",
    "                            torch.mean(critic_fake_pred)) + LAMBDA_GP * gp\n",
    "\n",
    "            # Gradient = 0\n",
    "            critic.zero_grad()\n",
    "\n",
    "            # Backprop. + Aufzeichnen dynamischen Graphen\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "\n",
    "            # Update Optimizer\n",
    "            critic_opt.step()\n",
    "\n",
    "        # Trainieren des Generators: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        gen_fake = critic(fake).reshape(-1)\n",
    "        gen_loss = -torch.mean(gen_fake)\n",
    "\n",
    "        # Gradient = 0\n",
    "        gen.zero_grad()\n",
    "\n",
    "        # Backprop.\n",
    "        gen_loss.backward()\n",
    "\n",
    "        # Update optimizer\n",
    "        gen_opt.step()\n",
    "\n",
    "        ## Visualization code ##\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            print(\n",
    "                f\"Step {cur_step}: Generator loss: {gen_loss}, critic loss: {critic_loss}\")\n",
    "            display_images(fake)\n",
    "            display_images(real_image)\n",
    "            gen_loss = 0\n",
    "            critic_loss = 0\n",
    "            saves_gen_samples(cur_step, fake_noise)\n",
    "        cur_step += 1\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57d879c1bab31ddce3f98747a90aac1ecdf0d747d4f9b6f921c92f41b19b21c9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
